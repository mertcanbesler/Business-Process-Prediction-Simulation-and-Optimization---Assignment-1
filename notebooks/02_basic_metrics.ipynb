{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301ca9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TUM Master\\Business Process Prediction, Simulation and Optimization\\Assignment 1\\.venv\\lib\\site-packages\\pm4py\\util\\dt_parsing\\parser.py:82: UserWarning: ISO8601 strings are not fully supported with strpfromiso for Python versions below 3.11\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5559a38dfd154873b12c4b71a349ad64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/31509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Event log loaded successfully with 1202267 events and 31509 cases.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ“‚ Load Event Log (Setup)\n",
    "# ============================================================\n",
    "\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define path to the .xes file (adjust if needed)\n",
    "xes_path = Path(\"../data/raw/BPI_Challenge_2017.xes\")\n",
    "\n",
    "# Load XES log using PM4Py\n",
    "log = xes_importer.apply(str(xes_path))\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "\n",
    "# Confirm successful load\n",
    "print(f\"âœ… Event log loaded successfully with {len(df)} events and {df['case:concept:name'].nunique()} cases.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d3cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 31509\n",
      "Number of events: 1202267\n",
      "Number of process variants: 15930\n",
      "Number of event labels: 26\n",
      "Number of process variants: 15930\n",
      "Number of event labels: 26\n",
      "Mean case length: 38.16 events\n",
      "Standard deviation of case length: 16.72 events\n",
      "Mean case length: 38.16 events\n",
      "Standard deviation of case length: 16.72 events\n",
      "Mean case duration: 21.90 days\n",
      "Standard deviation of case duration: 13.17 days\n",
      "Mean case duration: 21.90 days\n",
      "Standard deviation of case duration: 13.17 days\n",
      "Number of categorical event attributes: 12\n",
      "Mean inter-event time: 14 hours,\n",
      "Number of categorical event attributes: 12\n",
      "Mean inter-event time: 14 hours,\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š Basic Event Log Analysis (Metrics)\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings for cleaner output\n",
    "import pandas as pd\n",
    "\n",
    "# Number of cases\n",
    "num_cases = df['case:concept:name'].nunique()\n",
    "print(f\"Number of cases: {num_cases}\")\n",
    "\n",
    "# Number of events\n",
    "num_events = len(df)\n",
    "print(f\"Number of events: {num_events}\")\n",
    "\n",
    "# Number of process variants\n",
    "num_variants = df.groupby('case:concept:name').apply(lambda x: ' -> '.join(x['concept:name'])).nunique()\n",
    "print(f\"Number of process variants: {num_variants}\")\n",
    "\n",
    "# Number of case and event labels\n",
    "num_event_labels = df['concept:name'].nunique()\n",
    "print(f\"Number of event labels: {num_event_labels}\")\n",
    "\n",
    "# Mean and standard deviation of case length\n",
    "case_lengths = df.groupby('case:concept:name').size()\n",
    "mean_case_length = case_lengths.mean()\n",
    "std_case_length = case_lengths.std()\n",
    "print(f\"Mean case length: {mean_case_length:.2f} events\")\n",
    "print(f\"Standard deviation of case length: {std_case_length:.2f} events\")\n",
    "\n",
    "# Mean and standard deviation of case duration (in days)\n",
    "case_durations = df.groupby('case:concept:name').apply(lambda x: (x['time:timestamp'].max() - x['time:timestamp'].min()).total_seconds())\n",
    "mean_case_duration = case_durations.mean() / (24 * 3600)  # Convert seconds to days\n",
    "std_case_duration = case_durations.std() / (24 * 3600)  # Convert seconds to days\n",
    "print(f\"Mean case duration: {mean_case_duration:.2f} days\")\n",
    "print(f\"Standard deviation of case duration: {std_case_duration:.2f} days\")\n",
    "\n",
    "# Number of categorical event attributes\n",
    "categorical_attributes = df.select_dtypes(include=['object']).columns\n",
    "num_categorical_attributes = len(categorical_attributes)\n",
    "print(f\"Number of categorical event attributes: {num_categorical_attributes}\")\n",
    "\n",
    "# Mean Inter-Event Time\n",
    "inter_event_times = df.groupby('case:concept:name')['time:timestamp'].diff().dropna().dt.total_seconds()\n",
    "mean_inter_event_time = inter_event_times.mean()\n",
    "hours, remainder = divmod(mean_inter_event_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(f\"Mean inter-event time: {int(hours)} hours,\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ad591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rework rate: 0.1466 (14.66%)\n"
     ]
    }
   ],
   "source": [
    "#Rework rate calculation\n",
    "CASE, ACT, TS, LIFE = \"case:concept:name\", \"concept:name\", \"time:timestamp\", \"lifecycle:transition\"\n",
    "\n",
    "dfc = df[df[LIFE].astype(str).str.lower().eq(\"complete\")].copy()\n",
    "dfc[TS] = pd.to_datetime(dfc[TS], errors=\"coerce\")\n",
    "dfc = dfc.sort_values([CASE, TS], kind=\"mergesort\").drop_duplicates([CASE, ACT, TS])\n",
    "\n",
    "counts = dfc.groupby([CASE, ACT]).size()\n",
    "rework_events = (counts[counts > 1] - 1).sum()\n",
    "rework_rate = rework_events / len(dfc)\n",
    "\n",
    "print(f\"Rework rate: {rework_rate:.4f} ({rework_rate:.2%})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd90ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median throughput time: 19.09 days\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamps to datetime (safety check)\n",
    "df[\"time:timestamp\"] = pd.to_datetime(df[\"time:timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Calculate case durations in seconds\n",
    "case_durations = df.groupby(\"case:concept:name\")[\"time:timestamp\"].apply(lambda x: (x.max() - x.min()).total_seconds())\n",
    "\n",
    "# Convert seconds to days for interpretability\n",
    "case_durations_days = case_durations / (24 * 3600)\n",
    "\n",
    "# Compute median and mean throughput times\n",
    "median_throughput_time = case_durations_days.median()\n",
    "mean_throughput_time = case_durations_days.mean()\n",
    "\n",
    "print(f\"Median throughput time: {median_throughput_time:.2f} days\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
